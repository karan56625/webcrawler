package handlers

import (
	"fmt"
	"golang.org/x/net/html"
	"log"
	"net/http"
	"net/url"
	"os"
	"strconv"
	"strings"
	"sync"
	"webCrawler/config"
	"webCrawler/models"
)

// visited keeps track of which URLs have already been crawled.
var visited = make(map[string]bool)

// sitemap stores the entire site map generated by the crawler.
var sitemap *models.Node

// mu is a mutex to synchronize access to shared data (visited and sitemap).
var mu sync.Mutex

// wg is a wait group to track the completion of all crawl tasks.
var wg sync.WaitGroup

// CrawlHandler handles HTTP requests for the web crawler. It takes a target URL as a query parameter,
// starts the crawling process, and returns the sitemap.
func CrawlHandler(w http.ResponseWriter, r *http.Request) {

	var noOfWorker int
	var workerQueueLength int
	targetURL := r.URL.Query().Get("url")

	if targetURL == "" {
		http.Error(w, "Missing URL parameter", http.StatusBadRequest)
		return
	}
	u, err := url.Parse(targetURL)
	if err != nil {
		http.Error(w, "Invalid URL parameter", http.StatusBadRequest)
		return
	}
	noOfWorkerStr, exists := os.LookupEnv("NUMBER_OF_WORKER")
	if !exists {
		fmt.Println("NUMBER_OF_WORKER is not set")
		fmt.Println("Using default number of worker", config.DefaultNumberOfWorker)
		noOfWorker = config.DefaultNumberOfWorker
	} else {
		noOfWorker, err = strconv.Atoi(noOfWorkerStr)
		if err != nil {
			fmt.Printf("Invalid WORKER_COUNT value: %s\n", noOfWorkerStr)
			return
		}
	}

	queueLengthStr, exists := os.LookupEnv("WORKER_QUEUE_LENGTH")
	if !exists {
		fmt.Println("WORKER_QUEUE_LENGTH is not set")
		fmt.Println("Using default queue length", config.DefaultQueueLength)
		workerQueueLength = config.DefaultQueueLength
	} else {
		workerQueueLength, err = strconv.Atoi(queueLengthStr)
		if err != nil {
			fmt.Printf("Invalid WORKER_QUEUE_LENGTH value: %s\n", queueLengthStr)
			return
		}
	}
	// Create a buffered channel to limit concurrency
	queue := make(chan *url.URL, workerQueueLength)
	sitemap = new(models.Node)
	visited = make(map[string]bool)
	wg.Add(1)
	go func() {
		queue <- u
	}()

	// Start worker pool
	for i := 0; i < noOfWorker; i++ { // 10 workers
		go worker(queue, u.Host)
	}

	// Wait for all goroutines to finish
	wg.Wait()
	close(queue)

	fmt.Fprintln(w, formatSitemap(sitemap, u.Host))
}

// worker is a function that continuously processes URLs from the queue
// and passes them to the crawl function.
func worker(queue chan *url.URL, domain string) {
	for u := range queue {
		crawl(u, domain, queue)
	}
}

// crawl handles the actual crawling of a given URL, parsing its content, extracting links,
// and adding them to the sitemap. It also adds new URLs to the queue for further crawling.
func crawl(u *url.URL, domain string, queue chan *url.URL) {
	defer wg.Done()

	mu.Lock()
	if visited[u.String()] {
		mu.Unlock()
		return
	}

	visited[u.String()] = true
	mu.Unlock()

	resp, err := http.Get(u.String())
	if err != nil {
		log.Printf("Failed to crawl: %s\n", u.String())
		log.Println(err.Error())
		return
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		log.Printf("Non-200 response for: %s\n", u.String())
		return
	}

	doc, err := html.Parse(resp.Body)
	if err != nil {
		log.Printf("Failed to parse: %s\n", u.String())
		log.Println(err.Error())
		return
	}
	links := extractLinks(doc, u)
	for _, link := range links {
		nextURL, err := url.Parse(link)
		if err != nil || nextURL.Host != domain {
			continue
		}

		if !strings.HasPrefix(nextURL.Scheme, "http") {
			nextURL.Scheme = u.Scheme
		}

		wg.Add(1)
		go func() {
			queue <- nextURL
		}()
	}
}

func addNodeToSiteMap(uri string) {
	mu.Lock()
	if visited[uri] {
		mu.Unlock()
		return
	}
	visited[uri] = true
	resources := strings.Split(uri, "/")
	var lastNode = sitemap
	for _, resource := range resources {
		currentNode := findNode(lastNode, resource)
		if currentNode != nil {
			lastNode = currentNode
		} else {
			newNode := &models.Node{URI: resource}
			lastNode.Children = append(lastNode.Children, newNode)
			lastNode = newNode
		}
	}
	mu.Unlock()
}

// findNode searches for a node with a specific URI in the tree.
func findNode(node *models.Node, uri string) *models.Node {
	if node == nil {
		return nil
	}

	// Check if the current node matches the URI
	if node.URI == uri {
		return node
	}

	// Recursively search in the children
	for _, child := range node.Children {
		if found := findNode(child, uri); found != nil {
			return found
		}
	}

	return nil
}

// extractLinks recursively traverses the HTML nodes and extracts href links,
// resolving them relative to the base URL.
func extractLinks(n *html.Node, base *url.URL) []string {
	var links []string
	if n.Type == html.ElementNode && n.Data == "a" {
		for _, attr := range n.Attr {
			if attr.Key == "href" {
				link := resolveURL(attr.Val, base)
				if link != "" {
					addNodeToSiteMap(link)
					links = append(links, link)
				}
				break
			}
		}
	}

	for c := n.FirstChild; c != nil; c = c.NextSibling {
		links = append(links, extractLinks(c, base)...)
	}

	return links
}

// resolveURL resolves a relative URL against a base URL to produce an absolute URL.
func resolveURL(href string, base *url.URL) string {
	u, err := url.Parse(href)
	if err != nil || u.Host != "" && u.Host != base.Host {
		return ""
	}
	return base.ResolveReference(u).RequestURI()
}

// formatSitemap formats the sitemap into a readable string format for output.
func formatSitemap(sitemap *models.Node, domain string) string {
	var sb strings.Builder
	sb.WriteString(domain + "\n")

	createSiteMap(sitemap, &sb, 0)
	return sb.String()
}

func createSiteMap(node *models.Node, builder *strings.Builder, level int) {
	if node == nil {
		return
	}
	// Create indentation based on the current level
	indent := strings.Repeat("  ", level)
	builder.WriteString(fmt.Sprintf(indent + "- /" + node.URI + "\n"))

	// Recursively print each child
	for _, child := range node.Children {
		createSiteMap(child, builder, level+1)
	}
}
